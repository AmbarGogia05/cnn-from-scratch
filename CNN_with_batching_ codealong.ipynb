{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A nice way to understand how CNNs work is to build a simple model from scratch, which can be done using just Numpy. Here, we will go through the process and train the model on MNIST, structuring the model similarly to the original LeNet."
      ],
      "metadata": {
        "id": "B_AiEZGlTKHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this is meant to serve as a guide; just copying code will not get you anywhere. First, based on the concepts of CNN that you have learnt, try to complete the code with the hints given. The full code is there to guide you only if you get stuck."
      ],
      "metadata": {
        "id": "dLOGm66JdhnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every Convolutional Neural Network has a few fundamental layers -  convolutional layers, max-pooling layers, ReLU activation, and a dense neural net. Here, we start coding these layers, step by step. First, we write the forward pass for the convolutional layer"
      ],
      "metadata": {
        "id": "4jfHJ8LaTmCT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "529531f3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.out_channels = out_channels\n",
        "        # In this implementation we have assumed a square filter with both dimensions equalling kernel_size\n",
        "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)*0.001\n",
        "        self.bias = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First, we pad the input to maintain consistency in the input and output dimensions after applying the convolutional filter\n",
        "        pad = self.padding\n",
        "        stride = self.stride\n",
        "        k = self.kernel_size\n",
        "        weights = self.weights\n",
        "        # Assuming input x has shape (batch_size, in_channels, height, width)\n",
        "        batch_size, in_channels, dimx, dimy = x.shape\n",
        "        x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "        self.x = x\n",
        "        dimx_padded = x.shape[2]\n",
        "        dimy_padded = x.shape[3]\n",
        "        # We initialize the output matrix where we store the product values\n",
        "        out_channels = self.out_channels\n",
        "        out_height = (dimx_padded - k) // stride + 1\n",
        "        out_width = (dimy_padded - k) // stride + 1\n",
        "        output = np.zeros((batch_size, out_channels, out_height, out_width))\n",
        "\n",
        "        # We make slices of x, for which we use step size = stride and slice width as kernel size\n",
        "        # Think about the range for i and j while slicing, it is crucial to this step\n",
        "        # YOUR CODE GOES HERE to loop through batches, output channels, and spatial dimensions (height and width)\n",
        "            # YOUR CODE GOES HERE to extract the patch from the padded input\n",
        "            # To apply a convolution, we multiply element-wise each value of the filter matrix with the slice, and also add the bias afterwards\n",
        "            # YOUR CODE GOES HERE to compute the convolution for the current patch and weight/bias\n",
        "            # YOUR CODE GOES HERE to store the result in the output matrix\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.out_channels = out_channels\n",
        "        # In this implementation we have assumed a square filter with both dimensions equalling kernel_size\n",
        "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)*0.001\n",
        "        self.bias = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First, we pad the input to maintain consistency in the input and output dimensions after applying the convolutional filter\n",
        "        pad = self.padding\n",
        "        stride = self.stride\n",
        "        k = self.kernel_size\n",
        "        weights = self.weights\n",
        "        # Assuming input x has shape (batch_size, in_channels, height, width)\n",
        "        batch_size, in_channels, dimx, dimy = x.shape\n",
        "        x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
        "        self.x = x\n",
        "        dimx_padded = x.shape[2]\n",
        "        dimy_padded = x.shape[3]\n",
        "        # We initialize the output matrix where we store the product values\n",
        "        out_channels = self.out_channels\n",
        "        out_height = (dimx_padded - k) // stride + 1\n",
        "        out_width = (dimy_padded - k) // stride + 1\n",
        "        output = np.zeros((batch_size, out_channels, out_height, out_width))\n",
        "\n",
        "        # We make slices of x, for which we use step size = stride and slice width as kernel size\n",
        "        # Think about the range for i and j while slicing, it is crucial to this step\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(out_channels):\n",
        "                for i in range (0, dimx_padded-k+1, stride):\n",
        "                    for j in range(0, dimy_padded-k+1, stride):\n",
        "                        patch = x[b, :, i:i+k, j:j+k]\n",
        "                        # To apply a convolution, we multiply element-wise each value of the filter matrix with the slice, and also add the bias afterwards\n",
        "                        prod = weights[c_out, :, :, :] * patch\n",
        "                        val = np.sum(prod) + self.bias[c_out]\n",
        "                        output[b, c_out, i//stride, j//stride] = val\n",
        "        return output"
      ],
      "metadata": {
        "id": "Jvq5LIYwhJjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will move on to writing the forward pass for a max-pooling layer. This layer reduces the dimensionality of the output relative to the input by retaining information about the maximum values in each chunk. In a sense, it retains the most relevant information, balancing a tradeoff between information loss and the time required for processing.\n",
        "Note that we wish to store information about the indices of the maximum value in each chunk, as this will come to be useful during the backpropagation."
      ],
      "metadata": {
        "id": "Bm_Mln4FhKSl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a7aa513"
      },
      "source": [
        "import numpy as np\n",
        "# Here, we maxpool assuming a 3D array x\n",
        "class MaxPool:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        output_height = (height - kernel_size) // stride + 1\n",
        "        output_width = (width - kernel_size) // stride + 1\n",
        "        output = np.zeros((batch_size, channels, output_height, output_width))\n",
        "        self.max_mask = np.zeros_like(x)\n",
        "        self.max_indices = np.zeros((batch_size, channels, output_height, output_width, 2), dtype=int)\n",
        "\n",
        "        # YOUR CODE GOES HERE to loop through batches, channels, and spatial dimensions (height and width)\n",
        "            # YOUR CODE GOES HERE to extract the chunk from the input\n",
        "            # YOUR CODE GOES HERE to compute the maximum value in the chunk\n",
        "            # YOUR CODE GOES HERE to store the maximum value in the output matrix\n",
        "            # YOUR CODE GOES HERE to find the indices of the maximum value within the chunk\n",
        "            # YOUR CODE GOES HERE to store the mask and indices of the maximum value\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Here, we maxpool assuming a 3D array x\n",
        "class MaxPool:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        kernel_size = self.kernel_size\n",
        "        stride = self.stride\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        output_height = (height - kernel_size) // stride + 1\n",
        "        output_width = (width - kernel_size) // stride + 1\n",
        "        output = np.zeros((batch_size, channels, output_height, output_width))\n",
        "        self.max_mask = np.zeros_like(x)\n",
        "        self.max_indices = np.zeros((batch_size, channels, output_height, output_width, 2), dtype=int)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(0, height - kernel_size + 1, stride):\n",
        "                    for j in range(0, width - kernel_size + 1, stride):\n",
        "                        chunk = x[b, c, i:i + kernel_size, j:j + kernel_size]\n",
        "                        output[b, c, i // stride, j // stride] = np.max(chunk)\n",
        "                        flat_index = np.argmax(chunk)\n",
        "                        i_max, j_max = np.unravel_index(flat_index, chunk.shape)\n",
        "                        self.max_mask[b, c, i + i_max, j + j_max] = 1\n",
        "                        self.max_indices[b, c, i // stride, j // stride] = (i + i_max, j + j_max)\n",
        "        return output"
      ],
      "metadata": {
        "id": "cwbFVYgdjxZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we quickly define the ReLU class (as you shall see, defining it as a class is useful as it allows us to implement common forward pass and back-propagation methods through all the layers."
      ],
      "metadata": {
        "id": "I5gldbXPmdlS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8e9593a"
      },
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Here, x is a numpy array of arbitrary dimension\n",
        "        # YOUR CODE GOES HERE to apply the ReLU activation function\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "    # Here, x is a numpy array of arbitrary dimension\n",
        "        return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "4j9aaLY9mpbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we pass data to the dense neural network layer, it is flattened into a 1D array (or in the case of batching, a 2D array with a 1D array associated with each image in the batch). We now define a class that performs this function."
      ],
      "metadata": {
        "id": "unXm5WEBm_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        self.original_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_shape = x.shape\n",
        "        # The reshape should flatten the dimensions after the batch size\n",
        "        return x.reshape(x.shape[0], -1)"
      ],
      "metadata": {
        "id": "N-JaMc6AnYD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the flattened data is to be passed through a dense neural net layer. We define the class associated in the code below (the logic is the same as was covered in prior weeks).\n",
        "Please note, that while training the model, an issue was discovered that initializing weights as zeros or using the NumPy random function led to repeated warnings about exploding gradients. Hence, we took inspiration from the original LeNet (1998) paper and decided to initialize weights by selecting from a distribution as defined below. You are encouraged to try different initializations to explore the issues that arise."
      ],
      "metadata": {
        "id": "SB-WZnODnzui"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fff1b8d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DenseNNLayer:\n",
        "    def __init__(self, input_features, output_features):\n",
        "        self.in_features = input_features\n",
        "        self.out_features = output_features\n",
        "\n",
        "        # Initializing weights as zeros caused gradients to explode\n",
        "        # self.weights = np.zeros((self.out_features, self.in_features+1))\n",
        "        # self.weights = np.random.randn(self.out_features, self.in_features + 1) * 0.0001\n",
        "        limit = 1 / np.sqrt(self.in_features)\n",
        "        self.weights = np.random.uniform(-limit, limit, (self.out_features, self.in_features + 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights = self.weights\n",
        "        B = x.shape[0]\n",
        "        # YOUR CODE GOES HERE to add a bias term to the input (augmenting the input with a column of ones)\n",
        "        x_augmented = None # Replace with your code\n",
        "        self.x_augmented = x_augmented\n",
        "        # YOUR CODE GOES HERE to perform the matrix multiplication for the forward pass, considering the batch dimension\n",
        "        output = None # Replace with your code\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNNLayer:\n",
        "    def __init__(self, input_features, output_features):\n",
        "        self.in_features = input_features\n",
        "        self.out_features = output_features\n",
        "\n",
        "        # Initializing weights as zeros caused gradients to explode\n",
        "        # self.weights = np.zeros((self.out_features, self.in_features+1))\n",
        "        # self.weights = np.random.randn(self.out_features, self.in_features + 1) * 0.0001\n",
        "        limit = 1 / np.sqrt(self.in_features)\n",
        "        self.weights = np.random.uniform(-limit, limit, (self.out_features, self.in_features + 1))\n",
        "    def forward(self, x):\n",
        "        weights = self.weights\n",
        "        B = x.shape[0]\n",
        "        x_augmented = np.concatenate((np.ones((B, 1)), x), axis=1)\n",
        "        self.x_augmented = x_augmented\n",
        "        output = x_augmented @ weights.T #Here, x_augmented has dimension batchsize * input_features+1, weights.T has dimension input_features+1, output_features\n",
        "        return output"
      ],
      "metadata": {
        "id": "uzENafinoC2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need a loss function. Here, we use cross-entropy, which is common in classification problems. We need to first normalize the activations by passing them through a softmax layer."
      ],
      "metadata": {
        "id": "zQlrmaPIgggV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44d3e2eb"
      },
      "source": [
        "class SoftMax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x has dimensions (batch size, output features)\n",
        "        # YOUR CODE GOES HERE to apply the softmax function\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x has dimensions (batch size, output features)\n",
        "        x_shifted = x - np.max(x, axis=1, keepdims=True) # Doesn't change the result, just results in smaller exponentials preventing potential overflow issues\n",
        "        exp_x = np.exp(x_shifted)\n",
        "        probabilities = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "Fh7c_-5Yg3OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a side note, its important to understand broadcasting rules for NumPy here.\n",
        "https://medium.com/@weidagang/understanding-broadcasting-in-numpy-c44dceae42ea\n",
        "This is a great starting point to get clarity. Here, we had to use keepdims=True to get an output from np.max of dimension (batch_size, 1); otherwise, we would have a 1D array of length batch_size, which can not be broadcasted to x. A similar logic applies to probabilities."
      ],
      "metadata": {
        "id": "AhZ93rkNitYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we compute the cross-entropy loss for the batch, and take the mean as the loss function we are trying to minimize. The following link provides a refresher if needed.\n",
        "https://www.geeksforgeeks.org/machine-learning/what-is-cross-entropy-loss-function/"
      ],
      "metadata": {
        "id": "Gox603dkLo6k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15b91eb9"
      },
      "source": [
        "import numpy as np\n",
        "# Here, we calculate the cross entropy loss for the output from the dense layer after applying softmax\n",
        "class CrossEntropy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        #x is the array of predicted probabilities, while y consists of true labels\n",
        "        # We calculate the cross entropy loss for each sample in the batch and then take the mean\n",
        "        # YOUR CODE GOES HERE to calculate the cross entropy loss for each sample in the batch\n",
        "        cross_entropy = None  # Replace with your code\n",
        "        # YOUR CODE GOES HERE to take the mean across the batch\n",
        "        loss = None # Replace with your code\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x  # store for backward\n",
        "        self.y = y\n",
        "\n",
        "        # Small epsilon to prevent log(0)\n",
        "        eps = 1e-15\n",
        "        clipped_x = np.clip(x, eps, 1 - eps)  # optional but safer than log(x + eps)\n",
        "\n",
        "        # Compute per-sample loss: shape (B,)\n",
        "        loss_per_sample = -np.sum(y * np.log(clipped_x), axis=1)\n",
        "\n",
        "        # Mean over batch\n",
        "        loss = np.mean(loss_per_sample)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "9Iy2eobhjUF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first major milestone completed for a basic CNN - we have completed the forward pass for all the major layers that we needed. Now, we work on the backpropagation of gradients, which will be used for batch gradient descent."
      ],
      "metadata": {
        "id": "wdQIlvBbMS34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first gradient we compute is the gradient of the cross-entropy loss with respect to the output values (logits) provided to the softmax function. Note that we are combining two steps here - the gradient w.r.t. the cross-entropy loss function inputs, and the gradient of these inputs w.r.t the softmax input logits. This is because it greatly simplifies the mathematics involved, and we do not need the intervening gradients as there are no parameters to be tuned in this layer. This is explained in the derivation below."
      ],
      "metadata": {
        "id": "-mSwMIIaMkCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient of Cross-Entropy Loss with Softmax\n",
        "\n",
        "Let:\n",
        "- $z \\in \\mathbb{R}^C$: input logits (output of the neural net)\n",
        "- $y \\in \\mathbb{R}^C$: one-hot true label\n",
        "- $s = \\text{softmax}(z) \\in \\mathbb{R}^C$, where:\n",
        "\n",
        "$$\n",
        "s_i = \\frac{e^{z_i}}{\\sum_k e^{z_k}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Cross-Entropy Loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_i y_i \\log(s_i)\n",
        "$$\n",
        "\n",
        "Substitute $s_i$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_i y_i \\log\\left( \\frac{e^{z_i}}{\\sum_k e^{z_k}} \\right)\n",
        "= -\\sum_i y_i (z_i - \\log \\sum_k e^{z_k})\n",
        "= -\\sum_i y_i z_i + \\log \\sum_k e^{z_k}\n",
        "$$\n",
        "##### Note: $$\n",
        "\\sum_i y_i \\log \\sum_k e^{z_k} = (\\sum_i y_i) \\log \\sum_k e^{z_k} = \\log \\sum_k e^{z_k}\n",
        "$$\n",
        "---\n",
        "\n",
        "### Now Take Derivative w.r.t. $z_j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_j}\n",
        "= -y_j + \\frac{e^{z_j}}{\\sum_k e^{z_k}} = s_j - y_j\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Final Result:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z_j} = s_j - y_j\n",
        "}\n",
        "$$\n",
        "\n",
        "This elegant result combines the derivative of softmax and cross-entropy into one simple expression — used in all modern deep learning frameworks.\n",
        "\n",
        "---\n",
        "\n",
        "### For a batch of $B$ samples:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial z} = \\frac{1}{B}(S - Y)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $S \\in \\mathbb{R}^{B \\times C}$: softmax outputs\n",
        "- $Y \\in \\mathbb{R}^{B \\times C}$: one-hot labels"
      ],
      "metadata": {
        "id": "ymR_VmPHNwpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will write methods for the layers defined above and extend the classes defined."
      ],
      "metadata": {
        "id": "e2MAGOgYQsNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we start with the backpropagation for the Cross-Entropy + SoftMax layers."
      ],
      "metadata": {
        "id": "2C8E-TFXRbJt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eda1fa"
      },
      "source": [
        "def CrossEntropyBackward(self):\n",
        "  # YOUR CODE GOES HERE to compute the gradient of the cross-entropy loss with respect to the input logits\n",
        "  pass\n",
        "\n",
        "CrossEntropy.backward = CrossEntropyBackward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropyBackward(self):\n",
        "  return (self.x - self.y)/(self.x.shape[0])\n",
        "\n",
        "CrossEntropy.backward = CrossEntropyBackward"
      ],
      "metadata": {
        "id": "Ngy7SAjiSsnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftMaxBackward(self, dout):\n",
        "  # dout is the gradient received by the SoftMax layer from the CrossEntropy layer - in this case, it is already the gradient that the softmax layer should return\n",
        "  return dout\n",
        "\n",
        "SoftMax.backward = SoftMaxBackward"
      ],
      "metadata": {
        "id": "tiesEs7iUAvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we proceed to the backpropagation function for the dense neural net layer. This receives the gradients w.r.t. the output logits, and computes the gradients w.r.t. the weights and the input."
      ],
      "metadata": {
        "id": "2SU9NlXXWl2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivation: Gradient of a Matrix Product\n",
        "\n",
        "We want to compute the derivative of a matrix product:\n",
        "\n",
        "Let:\n",
        "- $A \\in \\mathbb{R}^{m \\times n}$\n",
        "- $B \\in \\mathbb{R}^{k \\times n}$\n",
        "- $Z = A B^T \\in \\mathbb{R}^{m \\times k}$\n",
        "- $\\mathcal{L}$ is a scalar loss function that depends on $Z$\n",
        "\n",
        "---\n",
        "\n",
        "### Goal:\n",
        "\n",
        "Compute the gradient of the loss with respect to $B$ and $A$:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial B}, \\frac{\\partial \\mathcal{L}}{\\partial A}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Derivation\n",
        "\n",
        "Let’s take a single entry from $Z$:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\sum_{l=1}^{n} A_{i,l} B_{j,l}\n",
        "= A_{i,:} \\cdot B_{j,:}^T\n",
        "$$\n",
        "\n",
        "Then by the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial B_{j,l}} = \\sum_{i=1}^{m} \\frac{\\partial \\mathcal{L}}{\\partial Z_{i,j}} \\cdot \\frac{\\partial Z_{i,j}}{\\partial B_{j,l}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial A_{i,l}} = \\sum_{j=1}^{k} \\frac{\\partial \\mathcal{L}}{\\partial Z_{i,j}} \\cdot \\frac{\\partial Z_{i,j}}{\\partial A_{i,l}}\n",
        "$$\n",
        "\n",
        "From earlier:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Z_{i,j}}{\\partial B_{j,l}} = A_{i,l}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial Z_{i,j}}{\\partial A_{i,l}} = B_{j,l}\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial B_{j,l}} = \\sum_{i=1}^{m} \\frac{\\partial \\mathcal{L}}{\\partial Z_{i,j}} \\cdot A_{i,l}\n",
        "$$\n",
        "\n",
        "This is equivalent to a matrix multiplication between:\n",
        "\n",
        "- Row $j$ of $(\\frac{\\partial \\mathcal{L}}{\\partial Z})^T$\n",
        "- Column $l$ of $A$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial A_{i,l}} = \\sum_{j=1}^{k} \\frac{\\partial \\mathcal{L}}{\\partial Z_{i,j}} \\cdot B_{j,l}\n",
        "$$\n",
        "\n",
        "This is equivalent to a matrix multiplication between:\n",
        "\n",
        "- Row $i$ of $\\frac{\\partial \\mathcal{L}}{\\partial Z}$\n",
        "- Column l of B\n",
        "\n",
        "---\n",
        "\n",
        "### Final Result (Matrix Form)\n",
        "\n",
        "Putting all entries together into a matrix:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial B} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial Z} \\right)^T A\n",
        "}\n",
        "$$\n",
        "####$$\n",
        "\\boxed{\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial A} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial Z} \\right) B\n",
        "}\n",
        "$$\n",
        "---\n",
        "\n",
        "This identity is widely used in neural networks when computing the gradient of a linear (dense) layer:\n",
        "\n",
        "$$\n",
        "Z = X W^T\n",
        "$$\n",
        "$$\n",
        "\\quad \\Rightarrow \\quad \\frac{\\partial \\mathcal{L}}{\\partial W} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial Z} \\right)^T X\n",
        "$$\n",
        "$$\n",
        "\\quad \\Rightarrow \\quad \\frac{\\partial \\mathcal{L}}{\\partial X} =  \\left( \\frac{\\partial \\mathcal{L}}{\\partial Z} \\right) W\n",
        "$$"
      ],
      "metadata": {
        "id": "pUBp9yyvAjRL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adc698b1"
      },
      "source": [
        "def DenseNNBackward(self, dout):\n",
        "    B = dout.shape[0]\n",
        "\n",
        "    # Gradient w.r.t. weights: (Out_features x Batch_size) @ (Batch_size x (In_features+1)) → (Out_features x In_features+1)\n",
        "    # YOUR CODE GOES HERE to compute the gradient of the loss with respect to the weights\n",
        "    self.grad_weights = None # Replace with your code\n",
        "\n",
        "    # Optional: Gradient clipping\n",
        "    norm = np.linalg.norm(self.grad_weights)\n",
        "    if norm > 1:\n",
        "        self.grad_weights *= (1 / norm)\n",
        "\n",
        "    # Gradient w.r.t. input: (Batch_size x Out_features) @ (Out_features x In_features+1) → (Batch_size x In_features+1)\n",
        "    # YOUR CODE GOES HERE to compute the gradient of the loss with respect to the augmented input\n",
        "    dx_aug = None # Replace with your code\n",
        "\n",
        "    # Drop bias column\n",
        "    return dx_aug[:, 1:]  # shape: (Batch_size x In_features)\n",
        "\n",
        "\n",
        "DenseNNLayer.backward = DenseNNBackward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DenseNNBackward(self, dout):\n",
        "    B = dout.shape[0]\n",
        "\n",
        "    # Gradient w.r.t. weights: (Out_features x Batch_size) @ (Batch_size x (In_features+1)) → (Out_features x In_features+1)\n",
        "    self.grad_weights = (dout.T @ self.x_augmented)\n",
        "\n",
        "    # Optional: Gradient clipping\n",
        "    norm = np.linalg.norm(self.grad_weights)\n",
        "    if norm > 1:\n",
        "        self.grad_weights *= (1 / norm)\n",
        "\n",
        "    # Gradient w.r.t. input: (Batch_size x Out_features) @ (Out_features x In_features+1) → (Batch_size x In_features+1)\n",
        "    dx_aug = dout @ self.weights  # same as dout · W\n",
        "\n",
        "    # Drop bias column\n",
        "    return dx_aug[:, 1:]  # shape: (Batch_size x In_features)\n",
        "\n",
        "\n",
        "DenseNNLayer.backward = DenseNNBackward"
      ],
      "metadata": {
        "id": "wwI89zClXC5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we, do the backprop through the flatten layer, which is pretty elementary."
      ],
      "metadata": {
        "id": "vQpKX0XbKRWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flattenBackward(self, dout):\n",
        "  return dout.reshape(self.original_shape)\n",
        "\n",
        "Flatten.backward = flattenBackward"
      ],
      "metadata": {
        "id": "_gRtDpV5KDCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we move on to the backprop through the ReLU activation function. It is simple to compute the derivative of the output w.r.t the input of the ReLU layer, and is left as an exercise."
      ],
      "metadata": {
        "id": "2PNSxgEnvudE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cafbd4d7"
      },
      "source": [
        "def ReLUBackward(self, dout):\n",
        "  # YOUR CODE GOES HERE to compute the gradient of the loss with respect to the input\n",
        "  pass\n",
        "\n",
        "ReLU.backward = ReLUBackward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLUBackward(self, dout):\n",
        "  return dout * (self.x>0)\n",
        "\n",
        "ReLU.backward = ReLUBackward"
      ],
      "metadata": {
        "id": "a5X2A3A7wIoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we wish to backpropagate through the MaxPool layer. An intuitive way to think about the derivatives needed, is that the output of the MaxPool layer equals the input for the positions that are the maximums in their respective chunks, while the output is independent of all other inputs. Now, it should be clear why we stored the indices of the maximum values of each chunk during the forward pass."
      ],
      "metadata": {
        "id": "32SM2SNjwnaR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db918a47"
      },
      "source": [
        "def MaxPoolBackward(self, dout):\n",
        "    dx = np.zeros_like(self.max_mask)  # same shape as input x\n",
        "\n",
        "    batch_size, channels, out_height, out_width = dout.shape\n",
        "\n",
        "    # YOUR CODE GOES HERE to iterate through the output and distribute the gradient to the correct input location\n",
        "        # YOUR CODE GOES HERE to get the stored maximum index\n",
        "        # YOUR CODE GOES HERE to assign the gradient from the output to the input at the maximum index\n",
        "\n",
        "    return dx\n",
        "\n",
        "MaxPool.backward = MaxPoolBackward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MaxPoolBackward(self, dout):\n",
        "    dx = np.zeros_like(self.max_mask)  # same shape as input x\n",
        "\n",
        "    batch_size, channels, out_height, out_width = dout.shape\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c in range(channels):\n",
        "            for i in range(out_height):\n",
        "                for j in range(out_width):\n",
        "                    i_max, j_max = self.max_indices[b, c, i, j]\n",
        "                    dx[b, c, i_max, j_max] = dout[b, c, i, j]\n",
        "\n",
        "    return dx\n",
        "\n",
        "MaxPool.backward = MaxPoolBackward"
      ],
      "metadata": {
        "id": "df11IhKFxube"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we are at the last step for the backprop - through the convolutional layer.\n",
        "### [CMU Explainer](https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf)\n",
        "\n",
        "You may want a different reference for more clarity, the document linked has a decent explanation, and we have tried to go through the entire process step-by-step below."
      ],
      "metadata": {
        "id": "1KxcNotxyN-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to walk through the process, step-by-step. We need to compute the gradients w.r.t. the input to the convolutional layer, w.r.t. the convolutional filters, and w.r.t. the bias terms. <br><br>\n",
        "Let us deal with a case with a $3 \\times 3$ input matrix $X$, and a filter F of dimension $ 2\\times 2$ with a stride of 1. The output in this case has dimension $2 \\times 2$. Let us write the terms of the output matrix $O$ explicitly. Note that the output has a bias added to each term.<br><br>\n",
        "$o_{11} = f_{11}x_{11}+f_{12}x_{12}+f_{21}x_{21}+f_{22}x_{22} + b$ <br>\n",
        "$o_{12} = f_{11}x_{12}+f_{12}x_{13}+f_{21}x_{22}+f_{22}x_{23} + b$ <br>\n",
        "$o_{21} = f_{11}x_{21}+f_{12}x_{22}+f_{21}x_{31}+f_{22}x_{32} + b$ <br>\n",
        "$o_{22} = f_{11}x_{22}+f_{12}x_{23}+f_{21}x_{32}+f_{22}x_{33} + b$ <br><br>\n",
        "Now, we are provided the derivative of the loss w.r.t. to the output matrix O, i.e. $\\frac{\\partial \\mathcal{L}}{\\partial o_{11}}, \\frac{\\partial \\mathcal{L}}{\\partial o_{12}}, \\frac{\\partial \\mathcal{L}}{\\partial o_{21}}, \\frac{\\partial \\mathcal{L}}{\\partial o_{22}}$ are known. We wish to compute the derivative w.r.t. $F$ and $X$, and the bias $b$. <br><br>\n",
        "Let us start with the derivative $\\frac{\\partial \\mathcal{L}}{\\partial b}$.\n",
        "From the chain rule, <br><br>\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i,j}\\frac{\\partial \\mathcal{L}}{\\partial o_{ij}}\\frac{\\partial o_{ij}}{\\partial b}, i, j = {1, 2}$$ <br>\n",
        "$$\\frac{\\partial o_{ij}}{\\partial b} = 1\\ ∀ i,j$$ <br>\n",
        "**So, the derivative of the loss function w.r.t. the bias is just the sum of the derivatives w.r.t. the output matrix terms.** <br><br>\n",
        "\n",
        "---\n",
        "\n",
        "Now let us compute the derivative of the loss function w.r.t. the **filter weights** \\( F \\).  \n",
        "Recall that the filter is:\n",
        "\n",
        "$$\n",
        "F =\n",
        "\\begin{bmatrix}\n",
        "f_{11} & f_{12} \\\\\n",
        "f_{21} & f_{22}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We want to compute:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{11}}, \\quad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{12}}, \\quad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{21}}, \\quad\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{22}}\n",
        "$$\n",
        "\n",
        "Let’s start with $ \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} $.  \n",
        "From the output expressions, we can see that $ f_{11} $ appears in all of these:\n",
        "\n",
        "- $ o_{11} = f_{11}x_{11} + \\dots $\n",
        "- $ o_{12} = f_{11}x_{12} + \\dots $\n",
        "- $ o_{21} = f_{11}x_{21} + \\dots $\n",
        "- $ o_{22} = f_{11}x_{22} + \\dots $\n",
        "\n",
        "So by the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{11}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot x_{11} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot x_{12} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot x_{21} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot x_{22}\n",
        "$$\n",
        "\n",
        "Similarly, for the other filter weights:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{12}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot x_{12} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot x_{13} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot x_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot x_{23}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{21}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot x_{21} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot x_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot x_{31} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot x_{32}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial f_{22}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot x_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot x_{23} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot x_{32} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot x_{33}\n",
        "$$\n",
        "\n",
        "So, the gradient of the loss with respect to each filter weight is just the sum of all the input pixels that were multiplied by it in the forward pass, each scaled by the gradient of the corresponding output.\n",
        "\n",
        "---\n",
        "\n",
        "Now let’s compute the derivative of the loss with respect to the **input matrix** $X$.\n",
        "\n",
        "Each input value contributes to multiple outputs. Let’s look at one example: $x_{22}$.\n",
        "\n",
        "It appears in:\n",
        "\n",
        "- $o_{11}$ via $f_{22}$  \n",
        "- $o_{12}$ via $f_{21}$  \n",
        "- $o_{21}$ via $f_{12}$  \n",
        "- $o_{22}$ via $f_{11}$  \n",
        "\n",
        "So the gradient is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{22}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot f_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot f_{21} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot f_{12} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot f_{11}\n",
        "$$\n",
        "\n",
        "Each input gradient is the sum of all the gradients it \"receives\" from outputs that used it, weighted by the filter values that used it.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let’s continue by computing the **gradient of the loss with respect to all values of the input matrix** $X$.\n",
        "\n",
        "We know from earlier that each $x_{ij}$ contributes to multiple outputs due to the sliding window, and therefore receives gradients from multiple positions in the output matrix, scaled by corresponding filter weights.\n",
        "\n",
        "We compute a few examples explicitly:\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{11}$\n",
        "\n",
        "$x_{11}$ only contributes to $o_{11}$ via $f_{11}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{11}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot f_{11}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{12}$\n",
        "\n",
        "$x_{12}$ contributes to:\n",
        "\n",
        "- $o_{11}$ via $f_{12}$\n",
        "- $o_{12}$ via $f_{11}$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{12}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot f_{12} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot f_{11}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{13}$\n",
        "\n",
        "$x_{13}$ only appears in $o_{12}$ via $f_{12}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{13}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot f_{12}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{21}$\n",
        "\n",
        "$x_{21}$ contributes to:\n",
        "\n",
        "- $o_{11}$ via $f_{21}$\n",
        "- $o_{21}$ via $f_{11}$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{21}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot f_{21} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot f_{11}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "###$x_{22}$\n",
        "\n",
        "Previously shown:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{22}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{11}} \\cdot f_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot f_{21} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot f_{12} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot f_{11}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{23}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{23}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{12}} \\cdot f_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot f_{12}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{31}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{31}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot f_{21}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{32}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{32}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{21}} \\cdot f_{22} +\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot f_{21}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### $x_{33}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x_{33}} =\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial o_{22}} \\cdot f_{22}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##Summary: Input Gradients as a Convolution\n",
        "\n",
        "What we just did manually can be **efficiently computed** by:\n",
        "\n",
        "- **Flipping the filter** horizontally and vertically\n",
        "- Performing a **full convolution** (i.e., stride 1 and zero-padding so the output is the same size as the input)\n",
        "\n",
        "Let:\n",
        "\n",
        "- $O'$ be the matrix of gradients of the loss with respect to output:  \n",
        "  $O' = \\frac{\\partial \\mathcal{L}}{\\partial O}$\n",
        "- $F^\\text{rot}$ be the 180°-rotated filter (i.e., flipped over both axes)\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial X} = O' * F^\\text{rot}\n",
        "$$\n",
        "\n",
        "Where $*$ denotes **full convolution**.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Filter Gradients as a Convolution\n",
        "\n",
        "The gradient w.r.t. the filter can also be expressed as:\n",
        "\n",
        "- A **valid convolution** of the input $X$ with the output gradient $O'$\n",
        "\n",
        "That is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial F} = X * O'\n",
        "$$\n",
        "\n",
        "Where this is a **valid convolution** (no padding).\n",
        "\n",
        "---\n",
        "\n",
        "Both gradients can be computed efficiently using built-in convolution functions during backpropagation.\n",
        "Let us now implement this using NumPy."
      ],
      "metadata": {
        "id": "_XaKr6sd2HV0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "440b7f45"
      },
      "source": [
        "def ConvBackward(self, dout):\n",
        "    batch_size, out_channels, out_height, out_width = dout.shape\n",
        "    _, in_channels, H_padded, W_padded = self.x.shape\n",
        "    k = self.kernel_size\n",
        "    stride = self.stride\n",
        "    pad = self.padding\n",
        "\n",
        "    # Bias gradient\n",
        "    # YOUR CODE GOES HERE to compute the bias gradient\n",
        "    self.grad_bias = None # Replace with your code\n",
        "    norm_b = np.linalg.norm(self.grad_bias)\n",
        "    if norm_b > 1:\n",
        "        self.grad_bias *= (1 / norm_b) #normalization to stabilize values while preserving direction\n",
        "\n",
        "    # Weight gradients\n",
        "    # YOUR CODE GOES HERE to compute the weight gradients\n",
        "    self.grad_weights = np.zeros_like(self.weights) # Replace with your code\n",
        "    norm_w = np.linalg.norm(self.grad_weights)\n",
        "    if norm_w > 1:\n",
        "        self.grad_weights *= (1 / norm_w)\n",
        "\n",
        "    # Gradient w.r.t. input\n",
        "    # YOUR CODE GOES HERE to compute the input gradient\n",
        "    dx = np.zeros_like(self.x) # Replace with your code\n",
        "\n",
        "    # Return only the center (unpadded) region that corresponds to the original unpadded input x\n",
        "    if pad > 0:\n",
        "        return dx[:, :, pad:-pad, pad:-pad]\n",
        "    else:\n",
        "        return dx\n",
        "\n",
        "Conv.backward = ConvBackward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvBackward(self, dout):\n",
        "    batch_size, out_channels, out_height, out_width = dout.shape\n",
        "    _, in_channels, H_padded, W_padded = self.x.shape\n",
        "    k = self.kernel_size\n",
        "    stride = self.stride\n",
        "    pad = self.padding\n",
        "\n",
        "    # Bias gradient\n",
        "    self.grad_bias = np.sum(dout, axis=(0, 2, 3)) #we add for the whole batch, and this can be treated as an average because when initializing dout, we had already divided by batch_size\n",
        "    norm_b = np.linalg.norm(self.grad_bias)\n",
        "    if norm_b > 1:\n",
        "        self.grad_bias *= (1 / norm_b) #normalization to stabilize values while preserving direction\n",
        "\n",
        "    # Weight gradients\n",
        "    # We implement a valid convolution as described above\n",
        "    self.grad_weights = np.zeros_like(self.weights)\n",
        "    for b in range(batch_size):\n",
        "        x_b = self.x[b]\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                i_start = i * stride\n",
        "                j_start = j * stride\n",
        "                patch = x_b[:, i_start:i_start + k, j_start:j_start + k]\n",
        "                for c in range(out_channels):\n",
        "                    self.grad_weights[c] += patch * dout[b, c, i, j]\n",
        "\n",
        "    norm_w = np.linalg.norm(self.grad_weights)\n",
        "    if norm_w > 1:\n",
        "        self.grad_weights *= (1 / norm_w)\n",
        "\n",
        "    # Gradient w.r.t. input\n",
        "    dx = np.zeros_like(self.x)\n",
        "\n",
        "    # Pad dout with (k - 1) for full conv (k is kernel size)\n",
        "    pad_amt = k - 1\n",
        "    dout_padded = np.pad(dout, ((0, 0), (0, 0), (pad_amt, pad_amt), (pad_amt, pad_amt)), mode='constant')\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c_in in range(in_channels):\n",
        "            for c_out in range(out_channels):\n",
        "                flipped_filter = np.flip(self.weights[c_out, c_in], axis=(0, 1))  # (k, k)\n",
        "                for i in range(H_padded):\n",
        "                    for j in range(W_padded):\n",
        "                        region = dout_padded[b, c_out, i:i + k, j:j + k]\n",
        "                        dx[b, c_in, i, j] += np.sum(region * flipped_filter)\n",
        "\n",
        "    # Return only the center (unpadded) region that corresponds to the original unpadded input x\n",
        "    if pad > 0:\n",
        "        return dx[:, :, pad:-pad, pad:-pad]\n",
        "    else:\n",
        "        return dx\n",
        "\n",
        "Conv.backward = ConvBackward"
      ],
      "metadata": {
        "id": "sOGE4_WSNr8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we are now ready with a full forward and backward pass through our network. 2 layers - convolutional layer and the dense NN layer have parameters that require updating using the calculated gradients. Let us quickly implement the updation, before finally setting up our model that is based on the original LeNet-5 architecture, and training it on the MNIST dataset."
      ],
      "metadata": {
        "id": "HA4yzEvnVTxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Conv_apply_gradients(self, lr):\n",
        "    self.weights -= lr * self.grad_weights\n",
        "    self.bias -= lr * self.grad_bias\n",
        "Conv.apply_gradients = Conv_apply_gradients\n",
        "\n",
        "def NN_apply_gradients(self, lr):\n",
        "  self.weights -= lr * self.grad_weights\n",
        "DenseNNLayer.apply_gradients = NN_apply_gradients"
      ],
      "metadata": {
        "id": "5SQkjfYXbWlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 Architecture Explained\n",
        "\n",
        "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun in 1998 for handwritten digit recognition (e.g., MNIST). It consists of 7 layers (excluding input), all with learnable parameters. The network follows a structured pipeline:\n",
        "1.\tInput Layer: Takes a $32 \\times 32$ grayscale image. (MNIST images are $28 \\times 28$, so they’re typically padded with zeros to match this size.)\n",
        "2.\tC1 - Convolutional Layer: Applies 6 filters of size $5 \\times 5$, resulting in 6 feature maps of size $28 \\times 28$. (No padding used, stride = 1.)\n",
        "3.\tS2 - Subsampling Layer: Performs average pooling over $2 \\times 2$ regions with stride 2, reducing each feature map to size $14 \\times 14$.\n",
        "4.\tC3 - Convolutional Layer: Applies 16 filters of size $5 \\times 5$. Not all filters are connected to all previous feature maps — this forms a sparse connection pattern. Output size: $10 \\times 10$ feature maps.\n",
        "5.\tS4 - Subsampling Layer: Again does average pooling, reducing to $5 \\times 5$ feature maps.\n",
        "6.\tC5 - Convolutional Layer: Applies 120 filters of size $5 \\times 5$, covering the entire $5 \\times 5$ input — each output is a single value → yielding a 120-dimensional vector.\n",
        "7.\tF6 - Fully Connected Layer: A dense layer with 84 units, followed by a final output layer with 10 neurons (for digit classes 0–9).\n",
        "\n",
        "LeNet-5 combines convolutional feature extraction with simple pooling and fully connected classification, laying the foundation for modern deep learning architectures.\n",
        "\n",
        "Here, we shall implement a similar structure, using maxpooling instead of average pooling."
      ],
      "metadata": {
        "id": "TER6Gt44cCwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load and pre-process the data."
      ],
      "metadata": {
        "id": "BlGJI-Gnd6In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize input to [0,1]\n",
        "train_X = train_X.astype(np.float32) / 255.0\n",
        "test_X = test_X.astype(np.float32) / 255.0\n",
        "\n",
        "# Add channel dimension\n",
        "train_X = train_X[:, None, :, :]   # shape: (B, 1, 28, 28)\n",
        "test_X = test_X[:, None, :, :]\n",
        "\n",
        "# Pad to (1, 32, 32) to match LeNet\n",
        "train_X = np.pad(train_X, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
        "test_X = np.pad(test_X, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
        "\n",
        "# One-hot encode labels\n",
        "train_y_oh = np.eye(10)[train_y]   # shape: (B, 10)\n",
        "test_y_oh = np.eye(10)[test_y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvg0Xi5CdWpS",
        "outputId": "8cdcdf16-25e8-4eba-daf5-6a3e41ecef63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we write a function to create batches."
      ],
      "metadata": {
        "id": "NbiTXwwheFK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(X, y, batch_size):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]"
      ],
      "metadata": {
        "id": "TVX8bSoyeCLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to define the model, based on LeNet architecture"
      ],
      "metadata": {
        "id": "HLud8GzYeJ3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = [\n",
        "    Conv(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
        "    MaxPool(kernel_size=2, stride=2),\n",
        "    Conv(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
        "    MaxPool(kernel_size=2, stride=2),\n",
        "    Flatten(),\n",
        "    DenseNNLayer(input_features=400, output_features=120),\n",
        "    ReLU(),\n",
        "    DenseNNLayer(input_features=120, output_features=84),\n",
        "    ReLU(),\n",
        "    DenseNNLayer(input_features=84, output_features=10),\n",
        "    SoftMax()\n",
        "]"
      ],
      "metadata": {
        "id": "x6XJEVivd8yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we write the training loop by creating batches, then performing forward and backward passes through the defined model."
      ],
      "metadata": {
        "id": "cgluVHvFfB1_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7d629fe"
      },
      "source": [
        "from tqdm import tqdm\n",
        "num_epochs = 5\n",
        "loss_fn = CrossEntropy()\n",
        "learning_rate = 0.01\n",
        "batch_size = 16\n",
        "num_samples = len(train_X)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Wrap batches with tqdm for progress bar\n",
        "    batch_iterator = tqdm(create_batches(train_X, train_y_oh, batch_size), total=num_samples//batch_size, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for x_batch, y_batch in batch_iterator:\n",
        "        # YOUR CODE GOES HERE for the forward pass\n",
        "        out = x_batch\n",
        "        for layer in model:\n",
        "            pass\n",
        "\n",
        "        # YOUR CODE GOES HERE to calculate the loss\n",
        "\n",
        "        # YOUR CODE GOES HERE for the backward pass\n",
        "        dout = loss_fn.backward()\n",
        "        for layer in reversed(model):\n",
        "            pass\n",
        "        # YOUR CODE GOES HERE to apply gradients\n",
        "        for layer in model:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # Update progress bar info\n",
        "        batch_iterator.set_postfix(loss=loss, acc=f\"{correct/total:.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1} completed - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "num_epochs = 5\n",
        "loss_fn = CrossEntropy()\n",
        "learning_rate = 0.01\n",
        "batch_size = 16\n",
        "num_samples = len(train_X)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Wrap batches with tqdm for progress bar\n",
        "    batch_iterator = tqdm(create_batches(train_X, train_y_oh, batch_size), total=num_samples//batch_size, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for x_batch, y_batch in batch_iterator:\n",
        "        out = x_batch\n",
        "        for layer in model:\n",
        "            out = layer.forward(out)\n",
        "\n",
        "        loss = loss_fn.forward(out, y_batch)\n",
        "        total_loss += loss * len(x_batch)\n",
        "\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        targets = np.argmax(y_batch, axis=1)\n",
        "        correct += np.sum(preds == targets)\n",
        "        total += len(x_batch)\n",
        "\n",
        "        dout = loss_fn.backward()\n",
        "        for layer in reversed(model):\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        for layer in model:\n",
        "            if hasattr(layer, \"apply_gradients\"):\n",
        "                layer.apply_gradients(learning_rate)\n",
        "\n",
        "        # Update progress bar info\n",
        "        batch_iterator.set_postfix(loss=loss, acc=f\"{correct/total:.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1} completed - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "x8-IBp-Ce6p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we write a testing function, to test our trained model."
      ],
      "metadata": {
        "id": "yiyS8ZMTgMu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X, y, batch_size=64):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    def create_batches(X, y, batch_size):\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "    for x_batch, y_batch in create_batches(X, y, batch_size):\n",
        "        out = x_batch\n",
        "        for layer in model:\n",
        "            out = layer.forward(out)\n",
        "\n",
        "        preds = np.argmax(out, axis=1)\n",
        "        targets = np.argmax(y_batch, axis=1)\n",
        "\n",
        "        correct += np.sum(preds == targets)\n",
        "        total += len(x_batch)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\" Test Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate(model, test_X, test_y_oh, batch_size=64)"
      ],
      "metadata": {
        "id": "QQNEReK-gQGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aaaand we're all done! Of course, we want to store the model parameters to be able to load it later."
      ],
      "metadata": {
        "id": "OqTFu8kFgyqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filename=\"lenet_weights.npz\"):\n",
        "    params = {}\n",
        "    for idx, layer in enumerate(model):\n",
        "        if hasattr(layer, \"weights\"):\n",
        "            params[f\"layer_{idx}_weights\"] = layer.weights\n",
        "        if hasattr(layer, \"bias\"):\n",
        "            params[f\"layer_{idx}_bias\"] = layer.bias\n",
        "    np.savez(filename, **params)\n",
        "    print(f\"✅ Model parameters saved to {filename}\")"
      ],
      "metadata": {
        "id": "iZpb8wnlg6nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, filename=\"lenet_weights.npz\"):\n",
        "    data = np.load(filename)\n",
        "    for idx, layer in enumerate(model):\n",
        "        if hasattr(layer, \"weights\"):\n",
        "            layer.weights = data[f\"layer_{idx}_weights\"]\n",
        "        if hasattr(layer, \"bias\"):\n",
        "            layer.bias = data[f\"layer_{idx}_bias\"]\n",
        "    print(f\"✅ Model parameters loaded from {filename}\")"
      ],
      "metadata": {
        "id": "-DhV8HD4g9j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, \"lenet_weights.npz\")"
      ],
      "metadata": {
        "id": "d12o1Y6ohDRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional - download the saved parameters as Colab clears its session data\n",
        "from google.colab import files\n",
        "files.download(\"lenet_weights.npz\")"
      ],
      "metadata": {
        "id": "EC6pJ8cLhHAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update - this model, while working perfectly well, is very slow to train. The slowness is due to the nested for loops used in the forward and back pass of the convolutional and maxpooling layers. The following is an excerpt from a conversation with ChatGPT:"
      ],
      "metadata": {
        "id": "ITjbvbysR8HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original implementation, convolution and pooling layers used nested Python loops to manually compute every patch-wise operation — iterating over the batch, channels, height, and width. This approach is highly inefficient in Python because each loop runs in pure Python (interpreted and slow), and repeated memory access (especially slicing) is costly. By contrast, vectorization leverages NumPy’s ability to perform large-scale matrix operations in compiled C under the hood, using SIMD (Single Instruction, Multiple Data) instructions that run on highly optimized BLAS libraries. For example, instead of looping over every patch to compute dot products between a filter and input, we use the im2col trick to reshape all patches into a matrix, flatten the filters, and compute all outputs at once using fast matrix multiplication (@). Similarly, for pooling, np.stride_tricks.as_strided enables efficient patch extraction in-place without copying memory, followed by np.max over axes to pool all patches simultaneously. Vectorization thus replaces thousands of slow iterations with a single batched operation, dramatically improving performance, especially for large datasets and deep models.\n"
      ],
      "metadata": {
        "id": "gj9l5yasSxGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to understand how one particular conversion of matrix to vector is taking place in the im2col function defined below. <br>\n",
        "First the input is padded as required to match the dimensions, and then the dimensions of the output are computed as out_h and out_w. <br>\n",
        "The cols array is used to extract and store all the patches from the original matrix (these patches have dimensions (K, K)), and the double for loop executes this task. This double loop extracts each shifted version of the input.\n",
        "For further explanation: <br>\n",
        "- i, j: position in the kernel\n",
        "-\tFor each kernel position (i, j), we take all KxK-sized patches that will align with that kernel position across all spatial locations.\n",
        "- Uses strided slicing to extract efficiently:\n",
        "- i:i+stride*out_h:stride picks every patch row\n",
        "-\tj:j+stride*out_w:stride picks every patch column\n",
        "Now, we\n",
        "1.\tReorder axes so each patch is first:\n",
        "\t-\tNew shape: (B, out_h, out_w, C, K, K)\n",
        "2.\tFlatten each patch into a row:\n",
        "\t-\tFinal shape: (B * out_h * out_w, C * K * K) — this is our im2col output.\n",
        "\n",
        "Similar ideas are used to construct the col2im function."
      ],
      "metadata": {
        "id": "jciy3FOXUBFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def im2col(x, kernel_size, stride, padding):\n",
        "    # Pad input: (B, C, H, W)\n",
        "    x_padded = np.pad(x, ((0,0), (0,0), (padding, padding), (padding, padding)))\n",
        "    B, C, H, W = x_padded.shape\n",
        "    K = kernel_size\n",
        "    out_h = (H - K) // stride + 1\n",
        "    out_w = (W - K) // stride + 1\n",
        "\n",
        "    cols = np.zeros((B, C, K, K, out_h, out_w))\n",
        "\n",
        "    for i in range(K):\n",
        "        for j in range(K):\n",
        "            cols[:, :, i, j, :, :] = x_padded[:, :, i:i+stride*out_h:stride, j:j+stride*out_w:stride]\n",
        "\n",
        "    cols = cols.transpose(0, 4, 5, 1, 2, 3).reshape(B*out_h*out_w, -1)\n",
        "    return cols\n",
        "\n",
        "def col2im(cols, x_shape, kernel_size, stride, padding):\n",
        "    B, C, H, W = x_shape\n",
        "    H_padded, W_padded = H + 2*padding, W + 2*padding\n",
        "    x_padded = np.zeros((B, C, H_padded, W_padded))\n",
        "    out_h = (H_padded - kernel_size) // stride + 1\n",
        "    out_w = (W_padded - kernel_size) // stride + 1\n",
        "\n",
        "    cols_reshaped = cols.reshape(B, out_h, out_w, C, kernel_size, kernel_size).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    for i in range(kernel_size):\n",
        "        for j in range(kernel_size):\n",
        "            x_padded[:, :, i:i+stride*out_h:stride, j:j+stride*out_w:stride] += cols_reshaped[:, :, i, j, :, :]\n",
        "\n",
        "    if padding > 0:\n",
        "        return x_padded[:, :, padding:-padding, padding:-padding]\n",
        "    else:\n",
        "        return x_padded\n"
      ],
      "metadata": {
        "id": "R9TWUgg9S_dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.out_channels = out_channels\n",
        "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
        "        self.bias = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        self.x_col = im2col(x, self.kernel_size, self.stride, self.padding)\n",
        "        self.w_col = self.weights.reshape(self.out_channels, -1)\n",
        "        B, C, H, W = x.shape\n",
        "        out_h = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "        out_w = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "        out = self.x_col @ self.w_col.T + self.bias\n",
        "        out = out.reshape(B, out_h, out_w, self.out_channels).transpose(0, 3, 1, 2)\n",
        "        return out\n",
        "\n",
        "def ConvBackward(self, dout):\n",
        "    B, C_out, H_out, W_out = dout.shape\n",
        "    dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, C_out)\n",
        "\n",
        "    # Bias gradient\n",
        "    self.grad_bias = np.sum(dout_reshaped, axis=0)\n",
        "    norm_b = np.linalg.norm(self.grad_bias)\n",
        "    if norm_b > 1:\n",
        "        self.grad_bias *= (1 / norm_b)\n",
        "\n",
        "    # Weight gradient\n",
        "    self.grad_weights = dout_reshaped.T @ self.x_col\n",
        "    self.grad_weights = self.grad_weights.reshape(self.out_channels, -1, self.kernel_size, self.kernel_size)\n",
        "    norm_w = np.linalg.norm(self.grad_weights)\n",
        "    if norm_w > 1:\n",
        "        self.grad_weights *= (1 / norm_w)\n",
        "\n",
        "    # Input gradient\n",
        "    w_flat = self.w_col\n",
        "    dx_col = dout_reshaped @ w_flat\n",
        "    dx = col2im(dx_col, self.x.shape, self.kernel_size, self.stride, self.padding)\n",
        "    return dx\n",
        "\n",
        "Conv.backward = ConvBackward"
      ],
      "metadata": {
        "id": "36r_aWVHT_X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we update the MaxPool layer to avoid nested for loops, and speed up processing. This time, we use a NumPy function that helps us avoid the issues we faced earlier."
      ],
      "metadata": {
        "id": "cQ5Kk7_1V_UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MaxPool:\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "        B, C, H, W = x.shape\n",
        "        K = self.kernel_size\n",
        "        S = self.stride\n",
        "\n",
        "        H_out = (H - K) // S + 1\n",
        "        W_out = (W - K) // S + 1\n",
        "\n",
        "        # Extract patches\n",
        "        patches = np.lib.stride_tricks.as_strided(\n",
        "            x,\n",
        "            shape=(B, C, H_out, W_out, K, K),\n",
        "            strides=(\n",
        "                x.strides[0],\n",
        "                x.strides[1],\n",
        "                x.strides[2] * S,\n",
        "                x.strides[3] * S,\n",
        "                x.strides[2],\n",
        "                x.strides[3],\n",
        "            ),\n",
        "            writeable=False\n",
        "        )  # (B, C, H_out, W_out, K, K)\n",
        "\n",
        "        # Store mask for backward\n",
        "        self.patches = patches\n",
        "        self.max_mask = (patches == np.max(patches, axis=(4,5), keepdims=True))\n",
        "\n",
        "        # Compute output\n",
        "        output = np.max(patches, axis=(4,5))\n",
        "        return output"
      ],
      "metadata": {
        "id": "yz74L6MzWDx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MaxPoolBackward(self, dout):\n",
        "    B, C, H_out, W_out = dout.shape\n",
        "    K = self.kernel_size\n",
        "    S = self.stride\n",
        "\n",
        "    # Initialize dx\n",
        "    dx = np.zeros(self.x_shape, dtype=dout.dtype)\n",
        "\n",
        "    # Expand dout to broadcast over (K, K)\n",
        "    dout_expanded = dout[:, :, :, :, None, None]  # shape: (B, C, H_out, W_out, 1, 1)\n",
        "\n",
        "    # Distribute dout to the max locations only\n",
        "    dx_patches = self.max_mask * dout_expanded  # same shape as (B, C, H_out, W_out, K, K)\n",
        "\n",
        "    # Now scatter dx_patches back to dx\n",
        "    for i in range(K):\n",
        "        for j in range(K):\n",
        "            dx[:, :, i::S, j::S] += dx_patches[:, :, :, :, i, j]\n",
        "\n",
        "    return dx\n",
        "MaxPool.backward = MaxPoolBackward"
      ],
      "metadata": {
        "id": "9GgPMOlDWZc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try running the code for yourself with and without this fix - the performance gap will be very clear. During testing, the initial version took over 6 hours to train, while the second version trained on 60000 training samples and for 5 epochs in 20 minutes."
      ],
      "metadata": {
        "id": "Q-rlIVZmWamu"
      }
    }
  ]
}